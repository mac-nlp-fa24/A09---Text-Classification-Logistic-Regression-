{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f804b7c7-26d8-4e66-869b-4a2c5127e790",
   "metadata": {},
   "source": [
    "# Text Classification and Logistic Regression\n",
    "\n",
    "This activity's learning goals are to work towards being able to...\n",
    "\n",
    "1. Identify and describe the input and output structure for text classification tasks\n",
    "2. Explain the differences between generative and discriminative models for classification tasks\n",
    "3. Define and implement Logistic Regression for classification\n",
    "4. Practice using PyTorch for doing machine learning\n",
    "\n",
    "## Brief Review - Classification and Logistic Regression\n",
    "\n",
    "Recall that text classification is a task where our model must assign a class/label $c \\in C$ to some text input $\\omega \\in \\Sigma^*$. Much like with Naive Bayes, we'll be solving this problem by having a lot of labeled training data, pairs $\\{(\\omega_1, c_1), \\dots (\\omega_n, c_n)\\}$, and using that data to learn the relationship between the data and the labels  (what we call *supervised machine learning*). \n",
    "\n",
    "Naive Bayes was an example of *generative* modeling --- we attempted to understand the process that generates the data (or, rather, features of the data) for each class and evaluating the likelihood of the data under each model. Logistic regression forgoes learning anything about the generative process and instead tries to map features of the data directly to class membership\n",
    "\n",
    "## Our Task Today\n",
    "Let's build a simple bag-of-words logistic regression model in pytorch! Pytorch is a library for doing machine learning in python that is standard for working with neural networks, and is more than capable to implementing something like logistic regression (which is, in practice, a very simple neural model).\n",
    "\n",
    "Now, how are we going to implement logistic regression. \n",
    "\n",
    "If we are using a bag-of-words model, we're going have features that count each word in our vocabulary. that means we'll have one feature for each word/token in our vocabulary. Since each vocab item can be mapped to some integer, we can store the count of word $i$ (feature $i$, or $f_i$) at index $i$ of a size $\\lvert V \\rvert$ vector $x$! \n",
    "\n",
    "Each of these is multiplied by a coefficient that we'll learn from our training data. We can think of this as another size $\\lvert V \\rvert$ vector $w$, and then (following Jurafsky and Martin) we can compute a score for our input $w \\cdot x$ and then add a learned *bias* term $b$, toss it into a sigmoid function $\\sigma$ and then get a probability out. Let's focus (for now) on developing that *forward pass* of our model.\n",
    "\n",
    "We're going to be discussing a lot of pytorch functions here, so make sure you have a tab with the [official documentation](https://pytorch.org/docs/2.5/) open so you can check the details of each function (and get familiar with navigating the docs!).\n",
    "\n",
    "### Data and Preprocessing\n",
    "\n",
    "First, let's figure out our vocab (i.e., *features*). Lets load our data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c07c0a8-35c9-41a3-9cbe-50e50719d73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./data/train/{}.txt\"\n",
    "\n",
    "with open(path.format(\"pos\")) as pos_data_f:\n",
    "    pos_data = pos_data_f.read().lower().split(\"\\n\")\n",
    "\n",
    "with open(path.format(\"neg\")) as neg_data_f:\n",
    "    neg_data = neg_data_f.read().lower().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11843e10-69f0-45ea-bb9d-dc23fbb208b5",
   "metadata": {},
   "source": [
    "and have *any* word that appears have it's count as a feature. This is exactly like bag-of-words for naive Bayes, but we'll store the counts as a *vector* (an array, essentially!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62197147-381d-492c-a402-593e350fa0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "vocab = list(set(\" \".join(pos_data + neg_data).split()))\n",
    "# Construct a map from words to indices into the vocab list\n",
    "w2idx = {w:i for (i, w) in enumerate(vocab)}\n",
    "\n",
    "train_data = []\n",
    "\n",
    "for label, data in enumerate([neg_data, pos_data]):\n",
    "    for sent in data:\n",
    "        x = torch.zeros(len(vocab))\n",
    "        for w in sent.split():\n",
    "            if w in w2idx:\n",
    "                x[w2idx[w]] += 1\n",
    "        train_data.append((x, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64c418e-77dc-4ea7-8dea-6947d4708da7",
   "metadata": {},
   "source": [
    "Let's slow down and see exactly what we're doing. First, for every example, we construct a *tensor* of zeros of size `len(vocab)`. `len(vocab)` is the number of tokens, and thus the number of features! A tensor in pytorch is a generalization of vectors/matrices/etc. A 1D tensor is a vector, a 2D tensor is a matrix, and so on. You can also think of these as N-dimensional arrays!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7688914-faf7-42da-a144-a04fe43157bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vocab))\n",
    "print(torch.zeros(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce62b0af-e054-48a3-8268-2c1a3ffc1e36",
   "metadata": {},
   "source": [
    "So we have a vector of 32708 features! \n",
    "The dictionary w2idx (word-to-index) is a map from a word to the index into this vector that counts the appearances of that word. The most-nested for-loop iterates through all of the tokens in the example and adds 1 to the corresponding index. We're essentially building a count dictionary, but in a tensor!\n",
    "\n",
    "So let's look at example 0, the first in `neg_data`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196650b8-a5bd-478a-9dfb-1a873706e824",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(neg_data[0].split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c18de61-dda2-494a-85d9-9973df5a979a",
   "metadata": {},
   "source": [
    "Let's verify we counted the number of appearances of *the* correctly (since neg_data begins train data, the first examples are the same!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2610c7-b8d8-4986-9dff-fab93af4f5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w2idx[\"the\"])\n",
    "\n",
    "x, l = train_data[0]\n",
    "print(x[w2idx[\"the\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bb4597-60a5-44c5-8726-e196710fce80",
   "metadata": {},
   "source": [
    "Verify that this is correct!\n",
    "\n",
    "### Building our Classfier Class\n",
    "\n",
    "Now let's design our logistic regression model. We do this in pytorch designing a class that inherits from `nn.module`, a parent class for all pytorch models. Our focus is on initalizing our model and writing a `forward` method that defines a forward pass of our model (i.e., how we would classify an example with a trained model!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c7eff7-b33e-4280-8b2f-f7c1b0b43f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class LogisticSentimentClassifier(nn.Module):\n",
    "    def __init__(self, vocab):\n",
    "        # Call the nn.Module constructor\n",
    "        super(LogisticSentimentClassifier, self).__init__()\n",
    "\n",
    "        # Run our constructor\n",
    "        self.vocab = vocab\n",
    "\n",
    "        # Wrapping in nn.Parameter indicates that these are our\n",
    "        # model's learned parameters!\n",
    "        self.w = nn.Parameter(torch.empty(len(vocab)))\n",
    "        self.b = nn.Parameter(torch.empty(1))\n",
    "\n",
    "        # Randomly initialize the parameters Don't worry about this\n",
    "        nn.init.uniform_(self.w, -1.0/np.sqrt(len(self.vocab)), 1.0/np.sqrt(len(self.vocab)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        logit = torch.dot(self.w, x) + self.b\n",
    "        return F.sigmoid(logit).view(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2e9ff2-c3ca-4c9d-8bbe-f0f6587dde9f",
   "metadata": {},
   "source": [
    "Now we could stop here, but if we're clever we can design our classifier so it can classify multiple things at once (through the power of matrix multiplication!). \n",
    "\n",
    "To do this, we need to think of our weight $w$ as a $\\lvert V \\rvert$ x 1 matrix and our input $X$ as a $N$ x $\\lvert V \\rvert$ (where $N$ is the number of inputs). That way doing an $X \\cdot w$ multiplication gives us an $N$ x 1 matrix (i.e., a vector with the predictions for all $N$ of the inputs!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdacaeb7-84a8-455e-8854-7fde148d87bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticSentimentClassifierBatched(LogisticSentimentClassifier):\n",
    "    def __init__(self, vocab):\n",
    "        # Call the nn.Module constructor\n",
    "        super(LogisticSentimentClassifier, self).__init__()\n",
    "\n",
    "        # Run our constructor\n",
    "        self.vocab = vocab\n",
    "\n",
    "        # Wrapping in nn.Parameter indicates that these are our\n",
    "        # model's learned parameters!\n",
    "        self.w = nn.Parameter(torch.empty(len(vocab), 1))\n",
    "        self.b = nn.Parameter(torch.empty(1))\n",
    "\n",
    "        # Randomly initialize the parameters\n",
    "        nn.init.uniform_(self.w, -1.0/np.sqrt(len(self.vocab)), 1.0/np.sqrt(len(self.vocab)))\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return F.sigmoid(torch.matmul(x,self.w) + self.b).view(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2ed218-8fd3-48fc-ab06-6dc525e3f84e",
   "metadata": {},
   "source": [
    "### Training a model\n",
    "\n",
    "Now we can build a `LogisticSentimentClassifier` or `logisticSentimentClassifierBatched` object, but it will be a bad classifier, because our parameters are all random! We need to learn the *right* values for each parameter! We do this through an optimization process:\n",
    "\n",
    "1. Make a prediction (or predictions) with a bad model\n",
    "2. Compute a measure of how poorly the model predicted (called a *loss function*)\n",
    "3. Determine how our parameters must change to make those predictions better (called a *backward pass*, or often *backpropagation*, the algorithm that efficiently computes this for large models)\n",
    "4. Adjust our parameters a small amount in a direction that would make those predictions better\n",
    "\n",
    "If you take a proper Machine Learning class, you'll learn all about the classic optimization method in this setting, Stochastic Gradient Descent. J&M has a solid introduction! For us, we can focus on using it in PyTorch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9668fc72-10d6-45c2-b65b-61fd9c8cc68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "logisticSentiment = LogisticSentimentClassifierBatched(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386a22ef-b6fd-4d75-bf59-f567057cbcba",
   "metadata": {},
   "source": [
    "We build an `Optimizer` object that will manage updating our model's parameters --- the coefficients of the regression!\n",
    "\n",
    "To do this, we need to specify a *learning rate*. This tells the model how fast we should change parameters to better fit our data. To high of a learning rate, and we'll overshoot (and might even get `nan`s as parameter values as we overflow!). Too small, and it will take a LOT of training examples to learn good values of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f496305f-e413-4f78-9b21-47f81862f81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001 \n",
    "\n",
    "optimizer = optim.SGD(logisticSentiment.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d19151e-a924-43ae-b3d1-2602af63d9c0",
   "metadata": {},
   "source": [
    "We also need to define a *loss function*. This tells us what to minimize in order to improve. Here, we want to use *negative log likelihood*.\n",
    "\n",
    "**Scan back to your reading to find the formula for NLL** (fun fact: an approximation of the *cross-entropy*) and write a method to compute it given the true label $y$ and the predicted probability of a positive label $\\hat{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a010867d-2916-44f6-bbed-db48297121e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(y, y_hat):\n",
    "    # TODO: Implement NLL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c79732-bb0a-456b-afde-02e773b67988",
   "metadata": {},
   "source": [
    "We can also take a look at the *parameters* of our model! These should be our coefficients $w$ and the bias term $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a71db5f-3928-4f4c-9b40-4cc91c1b0b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in logisticSentiment.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef0cde7-74e3-4677-883d-49e91a8570a6",
   "metadata": {},
   "source": [
    "Now we prep our training data! We could go example by example, but a problem is that this makes training a little *unstable*. When we nudge our parameter values for each example, they can jump around a lot to try to adjust to individual examples! \n",
    "\n",
    "Instead, we might want to adjust in **batches** based on the *average* loss. That way we optimize so that we predict the entire batch of examples better at each step! \n",
    "\n",
    "Since we build a classifier that can classify multiple things at once, the only thing to adjust is our data!\n",
    "\n",
    "If I want to put $N$ different $\\lvert V \\rvert$ training examples together into an $N$ x $\\lvert V \\rvert$ matrix, we can use `torch.stack`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f75494c-592e-402f-a3d5-5b4c60f79005",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([x for x, y in train_data[:3]])\n",
    "\n",
    "print(torch.stack([x for x, y in train_data[:3]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c25224-7fda-47bc-b746-ddeaaa2bfc6f",
   "metadata": {},
   "source": [
    "If that's hard to read, we can always look at the *size* of a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad85252a-155a-472f-9e32-16465c5b786c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.stack([x for x, y in train_data[:3]]).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b127f7-2a0e-4fec-96fc-fced0122b2d3",
   "metadata": {},
   "source": [
    "Now we have a batch of 3 examples! We can also do the same for the corresponding labels, but since they aren't yet a tensor, we can call torch.tensor to turn a list of them into one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bbaebe-a5c9-41cb-9f8a-54c60bbd5761",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([y for x,y in train_data[:3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b000be57-bdd7-4c17-9fc8-4e72e8939d6e",
   "metadata": {},
   "source": [
    "Let's turn our training data into a list of batches of size at-most `batch_size`, with each batch being a pair of an $N$ x $\\lvert V \\rvert$ input matrix X and an $N$-length vector of the correct labels for each of the $N$ examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fd6ce6-3617-43e4-85de-3d1075cc5421",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Tuple\n",
    "\n",
    "def batch(data : Iterable[Tuple[torch.Tensor, int]], batch_size : int) -> Iterable[Tuple[torch.Tensor, torch.Tensor]]:\n",
    "    # TODO\n",
    "    return []\n",
    "\n",
    "batch_size = 20 # A parameter you can change!\n",
    "\n",
    "batched_train_data = batch(train_data, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1ceb6f-716a-4215-9df5-c60e1c7da786",
   "metadata": {},
   "source": [
    "Now finally to our 4 step loop! \n",
    "\n",
    "For every pair of X (input) and Y (label), we predict, measure our loss, compute whether we should increase or decrease our parameters with `loss.backward()`, and then update our parameters accordingly with `optimizer.step()`. One small technical detail is that we need to begin or end each iteration with calls to `optimizer.zero_grad()`, which resets some internal information about how the parameters should be updated (their *gradients*, named such for calculus reasons!). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a56367f-7de6-476b-b68f-5bc15dcc1e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, Y in batched_train_data:\n",
    "    \n",
    "    # reset for the next iteration\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # predict\n",
    "    Y_hat = logisticSentiment(X)\n",
    "\n",
    "    loss = 0\n",
    "    \n",
    "    # how bad did we predict each thing in our batch\n",
    "    # This can be optimized further with an NLL function that works over tensors!\n",
    "    # do\n",
    "    for y, y_hat in zip(Y, Y_hat):\n",
    "        loss += nll(y, y_hat) \n",
    "\n",
    "    loss /= len(ys)\n",
    "    \n",
    "    # how should parameters be adjusted\n",
    "    loss.backward()\n",
    "\n",
    "    # adjust our parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3beef93-0cc7-4fc2-9202-f054f9e8e866",
   "metadata": {},
   "source": [
    "This should get us a somewhat better model! Let's see how good it is!\n",
    "\n",
    "Complete this implementation of an evaluation function that computes the accuracy of the classifier. You'll need to determine how to pick the label (positive/1 or negative/0) based on the probability the model gives out. This is where we use our *decision boundary* -- **reference the reading**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbb10f6-c281-4e44-a59d-9212eb5bc0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib \n",
    "\n",
    "def eval(model, data):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in data:\n",
    "            y_hat = None  # TODO: Fix\n",
    "            if y_hat == y:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "    return correct/total\n",
    "\n",
    "eval(logisticSentiment, train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa289dc1-f76f-4bbf-b564-79669a37c908",
   "metadata": {},
   "source": [
    "Initialization is random, so your model might be good? But chances are it's not too far from *chance* accuracy (~50\\%) for a binary classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac18414-1581-4112-ae25-21a106469668",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"./data/test/{}.txt\"\n",
    "\n",
    "with open(test_path.format(\"pos\")) as pos_data_f:\n",
    "    pos_test = pos_data_f.read().split(\"\\n\")\n",
    "\n",
    "with open(test_path.format(\"neg\")) as neg_data_f:\n",
    "    neg_test = neg_data_f.read().split(\"\\n\")\n",
    "\n",
    "test_data = []\n",
    "for label, data in enumerate([neg_test, pos_test]):\n",
    "    for sent in data:\n",
    "        x = torch.zeros(len(vocab))\n",
    "        for w in sent.split():\n",
    "            if w in w2idx:\n",
    "                x[w2idx[w]] += 1\n",
    "        test_data.append((x, label))\n",
    "\n",
    "eval(logisticSentiment, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464e88df-61b2-4fd7-925f-a6871a046d4d",
   "metadata": {},
   "source": [
    "I unfortunately cannot predict randomness here as well, but you're probably not doing too great? What should make the model better is more training time though! \n",
    "\n",
    "#### Training over Multiple Epochs\n",
    "\n",
    "We have only seen each example once so far, which is fine for something like Naive Bayes. However, when optimizing, we're in a constant tug of war with our parameters, and if our learning rate is reasonable we can likely do better by seeing the same data again. \n",
    "\n",
    "Each loop over the dataset during training is called an *epoch*, and we often like to have a bunch of them! Here, I'll ask you to *experiment*, but in practice there are a few strategies for deciding how many epochs to have\n",
    "\n",
    "1. There is a *practical* upper bound --- compute time, patience, etc.\n",
    "2. We can try and *converge*: We're trying to minimize the log-likelihood, so we can keep going until we have run into that upper bound or we stop improving.\n",
    "3. We don't have to use the log likelihood as a measure to stop early. In fact, this can be problematic if this causes us to overfit on our training data (think about MLE n-gram models!). We can also implement early stopping based on whether or not log-likelihood over a *validation set* stops improving.\n",
    "4. There are even fancier strategies that, instead of stopping, modify the learning rate with the intuition that a smaller learning rate can help us continue to improve our model! *Learning rate scheduling* is fancy, but often useful for getting models that work just a little bit better!\n",
    "\n",
    "That aside, here I give you a template for going over 100 epochs (not bad for our tiny corpus). Once you fill in the blanks inside of the loop, start playing around with different numbers. Do you overfit? Do you converge within time? Does loss correlate neatly with accuracy? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff671052-e672-4cd3-9efb-ef25d9344bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    random.shuffle(train_data)\n",
    "    total_loss = 0\n",
    "    batched_train_data = batch(train_data, batch_size)\n",
    "\n",
    "    for X, Y in batched_train_data:    \n",
    "        # reset for the next iteration\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # predict\n",
    "        Y_hat = logisticSentiment(X)\n",
    "    \n",
    "        loss = 0\n",
    "        \n",
    "        # how bad did we predict (batched!)\n",
    "        for y, y_hat in zip(Y, Y_hat):\n",
    "            loss += nll(y, y_hat) \n",
    "    \n",
    "        loss /= len(ys)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # how should parameters be adjusted\n",
    "        loss.backward()\n",
    "    \n",
    "        # adjust our parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    # Log every 5 epochs!\n",
    "    if epoch % 5 == 0:\n",
    "        print(\"epoch {} | train loss: {:.5} | train acc: {:.3} | test acc: {:.3}\".format(epoch, \n",
    "                                                                                         total_loss/batch_count, \n",
    "                                                                                         eval(logisticSentiment, train_data), \n",
    "                                                                                         eval(logisticSentiment, test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3d0b96-33e8-43b5-a09c-47bbbf8fcac5",
   "metadata": {},
   "source": [
    "#### Bonus: Everything's a batch!\n",
    "\n",
    "Since we're using a small model like logistic regression and our dataset is also very small, we could actually go extreme with batching and made parameter updates based on the predictions of our model over the entire corpus. Try that out: You won't need an inner loop at all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1afc0d-a680-4d3c-a0bf-dd123cb63580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reinitialize so we're not starting with a trained model\n",
    "logisticSentiment = LogisticSentimentClassifierBatched(vocab)\n",
    "\n",
    "for epoch in range(1,301):\n",
    "    random.shuffle(train_data)\n",
    "\n",
    "    X = None # TODO!\n",
    "    Y = None # TODO!\n",
    "    \n",
    "    # reset for the next iteration\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # predict\n",
    "    Y_hat = logisticSentiment(X)\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    # how bad did we predict (over the full training set!)\n",
    "    for y, y_hat in zip(Y, Y_hat):\n",
    "        loss += nll(y, y_hat) \n",
    "    \n",
    "    loss /= len(ys)\n",
    "    \n",
    "    # how should parameters be adjusted\n",
    "    loss.backward()\n",
    "\n",
    "    # adjust our parameters\n",
    "    optimizer.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
